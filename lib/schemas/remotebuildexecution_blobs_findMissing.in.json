{
    "type": "object",
    "properties": {
        "instanceName": {
            "required": true,
            "type": "string"
        },
        "access_token": {
            "type": "string"
        },
        "alt": {
            "type": "string",
            "enum": [
                "json",
                "media",
                "proto"
            ],
            "default": "json"
        },
        "callback": {
            "type": "string"
        },
        "fields": {
            "type": "string"
        },
        "key": {
            "type": "string"
        },
        "oauth_token": {
            "type": "string"
        },
        "prettyPrint": {
            "type": "boolean",
            "default": true
        },
        "quotaUser": {
            "type": "string"
        },
        "uploadType": {
            "type": "string"
        },
        "upload_protocol": {
            "type": "string"
        },
        "requestBody": {
            "description": "A request message for\nContentAddressableStorage.FindMissingBlobs.",
            "properties": {
                "blobDigests": {
                    "description": "A list of the blobs to check.",
                    "items": {
                        "description": "A content digest. A digest for a given blob consists of the size of the blob\nand its hash. The hash algorithm to use is defined by the server, but servers\nSHOULD use SHA-256.\n\nThe size is considered to be an integral part of the digest and cannot be\nseparated. That is, even if the `hash` field is correctly specified but\n`size_bytes` is not, the server MUST reject the request.\n\nThe reason for including the size in the digest is as follows: in a great\nmany cases, the server needs to know the size of the blob it is about to work\nwith prior to starting an operation with it, such as flattening Merkle tree\nstructures or streaming it to a worker. Technically, the server could\nimplement a separate metadata store, but this results in a significantly more\ncomplicated implementation as opposed to having the client specify the size\nup-front (or storing the size along with the digest in every message where\ndigests are embedded). This does mean that the API leaks some implementation\ndetails of (what we consider to be) a reasonable server implementation, but\nwe consider this to be a worthwhile tradeoff.\n\nWhen a `Digest` is used to refer to a proto message, it always refers to the\nmessage in binary encoded form. To ensure consistent hashing, clients and\nservers MUST ensure that they serialize messages according to the following\nrules, even if there are alternate valid encodings for the same message:\n\n* Fields are serialized in tag order.\n* There are no unknown fields.\n* There are no duplicate fields.\n* Fields are serialized according to the default semantics for their type.\n\nMost protocol buffer implementations will always follow these rules when\nserializing, but care should be taken to avoid shortcuts. For instance,\nconcatenating two messages to merge them may produce duplicate fields.",
                        "properties": {
                            "hash": {
                                "description": "The hash. In the case of SHA-256, it will always be a lowercase hex string\nexactly 64 characters long.",
                                "type": "string"
                            },
                            "sizeBytes": {
                                "description": "The size of the blob, in bytes.",
                                "format": "int64",
                                "type": "string"
                            }
                        },
                        "type": "object"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        }
    }
}